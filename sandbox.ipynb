{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model_name='gpt-4o', api_key=api_key, temperature=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='3 multiplied by 12 equals 36.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 15, 'total_tokens': 24}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, id='run-7fc1f1a8-6334-4d2c-9f12-fa906d198621-0', usage_metadata={'input_tokens': 15, 'output_tokens': 9, 'total_tokens': 24})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = \"What is 3 * 12?\"\n",
    "\n",
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of claims:  5\n",
      "Speaker: Trump\n",
      "Claim: - The speaker asserts that the new technology will revolutionize the industry.\n",
      "Source Timestamp: 0.09\n",
      "Timestamp: 2024-07-17T23:13:00.964854\n",
      "Measurable: No\n",
      "Analysis: The term 'revolutionize' is too vague and subjective to measure. It lacks specific metrics or criteria that can be quantified or validated using real-world data.\n",
      "\n",
      "Speaker: Trump\n",
      "Claim: - The speaker claims that the technology will significantly reduce costs for businesses.\n",
      "Source Timestamp: 0.09\n",
      "Timestamp: 2024-07-17T23:13:00.964854\n",
      "Measurable: Yes\n",
      "Analysis: This claim can be measured by comparing the costs incurred by businesses before and after the implementation of the technology. Specific metrics could include operational costs, production costs, and overall expenditure. Data sources could include financial reports, cost analysis studies, and business performance metrics.\n",
      "\n",
      "Speaker: Trump\n",
      "Claim: - The speaker states that the technology will improve efficiency and productivity.\n",
      "Source Timestamp: 0.09\n",
      "Timestamp: 2024-07-17T23:13:00.964854\n",
      "Measurable: Yes\n",
      "Analysis: Efficiency and productivity can be quantified using metrics such as output per hour, process completion times, and resource utilization rates. Data sources could include productivity reports, time-tracking systems, and efficiency audits conducted before and after the technology implementation.\n",
      "\n",
      "Speaker: Trump\n",
      "Claim: - The speaker mentions that the technology has the potential to create new job opportunities.\n",
      "Source Timestamp: 0.09\n",
      "Timestamp: 2024-07-17T23:13:00.964854\n",
      "Measurable: Yes\n",
      "Analysis: This claim can be measured by tracking employment rates and job creation statistics in industries where the technology is implemented. Data sources could include labor market reports, employment statistics from government agencies, and job postings related to the new technology.\n",
      "\n",
      "Speaker: Trump\n",
      "Claim: - The speaker believes that the technology will enhance customer satisfaction.\n",
      "Source Timestamp: 0.09\n",
      "Timestamp: 2024-07-17T23:13:00.964854\n",
      "Measurable: Yes\n",
      "Analysis: Customer satisfaction can be quantified using metrics such as customer satisfaction scores (CSAT), Net Promoter Score (NPS), and customer feedback surveys. Data sources could include customer satisfaction surveys, reviews, and feedback collected before and after the technology implementation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "class Claims:\n",
    "    def __init__(self, speaker:str, claim:str, timestamp:str, measurable:bool, analysis:str, source_timestamp:str)->None:\n",
    "        self.speaker = speaker\n",
    "        self.claim = claim\n",
    "        self.measurable = measurable\n",
    "        self.analysis = analysis\n",
    "        self.timestamp = timestamp\n",
    "        self.source_timestamp = source_timestamp\n",
    "\n",
    "# Define Pydantic models\n",
    "class ClaimModel(BaseModel):\n",
    "    claim: str\n",
    "    measurable: bool\n",
    "    analysis: str\n",
    "    source_timestamp: str\n",
    "\n",
    "class ClaimList(BaseModel):\n",
    "    claims: List[ClaimModel]\n",
    "\n",
    "# Create the output parser\n",
    "parser = PydanticOutputParser(pydantic_object=ClaimList)\n",
    "\n",
    "# Load the document\n",
    "file_path = \"data/NEW_Trump_Pledges_To_Shut_Down_Department_Of_Education_At_Faith__Freedom_Event__FULL_SPEECH_2024-06-22T203001Z.csv\"\n",
    "loader = CSVLoader(file_path)\n",
    "document = loader.load()\n",
    "\n",
    "\n",
    "# Create a prompt to extract speaker from filename\n",
    "extract_speaker_template = ChatPromptTemplate.from_template(\n",
    "    \"Extract the speaker's name from this filename: {filename}. Only return the name, nothing else.\"\n",
    ")\n",
    "\n",
    "# Extract speaker using LLM\n",
    "speaker_chain = extract_speaker_template | llm\n",
    "speaker_result = speaker_chain.invoke({\"filename\": os.path.basename(file_path)})\n",
    "speaker = speaker_result.content.strip()\n",
    "\n",
    "# Get current timestamp\n",
    "timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "# Create the prompt templates for claim extraction and analysis\n",
    "extract_claims_template = ChatPromptTemplate.from_template(\n",
    "    \"Extract the most impactful claims made from the following text:\\n\\n{text}\\n\\n List each claim as a separate bullet point. After each claim, include in parentheses () the timestamp from the text that was used to derive the claim. Claims:\"\n",
    ")\n",
    "\n",
    "analyze_claims_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For each claim, determine if it is measurable or not. If measurable, explain how it could be quantifiably measured or validated using real-world data. If not measurable, explain why it's too vague or subjective to measure. Consider specific metrics, data sources, or methods that could be used for validation.\n",
    "\n",
    "    Format your response as a list of claims with their analysis, following this structure:\n",
    "    {format_instructions}\n",
    "\n",
    "    Claims to analyze:\n",
    "    {claims}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create the LCEL chain for claim extraction and analysis\n",
    "chain = (\n",
    "    {\"text\": lambda x: x['text']}\n",
    "    | extract_claims_template\n",
    "    | llm\n",
    "    | (lambda x: {\"claims\": [{\"claim\": c.split(\"(\")[0], \"source_timestamp\": c.split(\"(\")[1].split(\")\")[0]} for c in x.content.split(\"\\n\") if \"(\" in c and \")\" in c]})\n",
    "    | analyze_claims_template.partial(format_instructions=parser.get_format_instructions())\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"text\": document[0].page_content})\n",
    "\n",
    "# Convert the results to Claims objects\n",
    "claims_objects = [\n",
    "    Claims(\n",
    "        speaker=speaker,\n",
    "        claim=claim.claim,\n",
    "        timestamp=timestamp,\n",
    "        measurable=claim.measurable,\n",
    "        analysis=claim.analysis,\n",
    "        source_timestamp=claim.source_timestamp\n",
    "    )\n",
    "    for claim in result.claims\n",
    "]\n",
    "\n",
    "print(\"number of claims: \", len(claims_objects))\n",
    "# Print the results\n",
    "for claim in claims_objects:\n",
    "    print(f\"Speaker: {claim.speaker}\")\n",
    "    print(f\"Claim: {claim.claim}\")\n",
    "    print(f\"Source Timestamp: {claim.source_timestamp}\")\n",
    "    print(f\"Timestamp: {claim.timestamp}\")\n",
    "    print(f\"Measurable: {'Yes' if claim.measurable else 'No'}\")\n",
    "    print(f\"Analysis: {claim.analysis}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.output_parsers import PydanticOutputParser\n",
    "# from pydantic import BaseModel, Field\n",
    "# from typing import List\n",
    "# import datetime\n",
    "# import os\n",
    "\n",
    "# class Claims:\n",
    "#     def __init__(self, speaker:str, claim:str, timestamp:str, measurable:bool, analysis:str,quote:str)->None:\n",
    "#         self.speaker = speaker\n",
    "#         self.claim = claim\n",
    "#         self.measurable = measurable\n",
    "#         self.analysis = analysis\n",
    "#         self.timestamp = timestamp\n",
    "#         self.quote = quote\n",
    "\n",
    "# # Define Pydantic models\n",
    "# class ClaimModel(BaseModel):\n",
    "#     claim: str\n",
    "#     measurable: bool\n",
    "#     analysis: str\n",
    "#     quote: str\n",
    "\n",
    "# class ClaimList(BaseModel):\n",
    "#     claims: List[ClaimModel]\n",
    "\n",
    "# # Create the output parser\n",
    "# parser = PydanticOutputParser(pydantic_object=ClaimList)\n",
    "\n",
    "# # Load the document\n",
    "# file_path = \"data/President_Bidens_State_of_the_Union_Address_2024-03-08T034913Z.txt\"\n",
    "# loader = TextLoader(file_path)\n",
    "# document = loader.load()\n",
    "\n",
    "# # Create an LLM\n",
    "# # llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# # Create a prompt to extract speaker from filename\n",
    "# extract_speaker_template = ChatPromptTemplate.from_template(\n",
    "#     \"Extract the speaker's name from this filename: {filename}. Only return the name, nothing else.\"\n",
    "# )\n",
    "\n",
    "# # Extract speaker using LLM\n",
    "# speaker_chain = extract_speaker_template | llm\n",
    "# speaker_result = speaker_chain.invoke({\"filename\": os.path.basename(file_path)})\n",
    "# speaker = speaker_result.content.strip()\n",
    "\n",
    "# # Get current timestamp\n",
    "# timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "# # Create the prompt templates for claim extraction and analysis\n",
    "# extract_claims_template = ChatPromptTemplate.from_template(\n",
    "#     \"Extract the most impactful claims made from the following text:\\n\\n{text}\\n\\n List each claim as a separate bullet point. After each claim, include in parentheses () the partial quote from the text that was used to derive the claim. Claims:\"\n",
    "# )\n",
    "\n",
    "# analyze_claims_template = ChatPromptTemplate.from_template(\n",
    "#     \"\"\"For each claim, determine if it is measurable or not. If measurable, explain how it could be quantifiably measured or validated using real-world data. If not measurable, explain why it's too vague or subjective to measure. Consider specific metrics, data sources, or methods that could be used for validation.\n",
    "\n",
    "#     Format your response as a list of claims with their analysis, following this structure:\n",
    "#     {format_instructions}\n",
    "\n",
    "#     Claims to analyze:\n",
    "#     {claims}\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# # Create the LCEL chain for claim extraction and analysis\n",
    "# chain = (\n",
    "#     {\"text\": lambda x: x['text']}\n",
    "#     | extract_claims_template\n",
    "#     | llm\n",
    "#     | (lambda x: {\"claims\": [{\"claim\": c.split(\"(\")[0], \"quote\": c.split(\"(\")[1].split(\")\")[0]} for c in x.content.split(\"\\n\") if \"(\" in c and \")\" in c]})\n",
    "#     | analyze_claims_template.partial(format_instructions=parser.get_format_instructions())\n",
    "#     | llm\n",
    "#     | parser\n",
    "# )\n",
    "\n",
    "# # Run the chain\n",
    "# result = chain.invoke({\"text\": document[0].page_content})\n",
    "\n",
    "# # Convert the results to Claims objects\n",
    "# claims_objects = [\n",
    "#     Claims(\n",
    "#         speaker=speaker,\n",
    "#         claim=claim.claim,\n",
    "#         timestamp=timestamp,\n",
    "#         measurable=claim.measurable,\n",
    "#         analysis=claim.analysis,\n",
    "#         quote=claim.quote\n",
    "#     )\n",
    "#     for claim in result.claims\n",
    "# ]\n",
    "\n",
    "# print(\"nunber of claims: \", len(claims_objects))\n",
    "# # Print the results\n",
    "# for claim in claims_objects:\n",
    "#     print(f\"Speaker: {claim.speaker}\")\n",
    "#     print(f\"Claim: {claim.claim}\")\n",
    "#     print(f\"Quote: {claim.quote}\")\n",
    "#     print(f\"Timestamp: {claim.timestamp}\")\n",
    "#     print(f\"Measurable: {'Yes' if claim.measurable else 'No'}\")\n",
    "#     print(f\"Analysis: {claim.analysis}\")\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nunber of claims:  5\n"
     ]
    }
   ],
   "source": [
    "print(\"nunber of claims: \", len(claims_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.output_parsers import PydanticOutputParser\n",
    "# from pydantic import BaseModel, Field\n",
    "# from typing import List\n",
    "# import datetime\n",
    "# import os\n",
    "\n",
    "# class Claims:\n",
    "#     def __init__(self, speaker:str, claim:str, timestamp:str, measurable:bool, analysis:str)->None:\n",
    "#         self.speaker = speaker\n",
    "#         self.claim = claim\n",
    "#         self.measurable = measurable\n",
    "#         self.analysis = analysis\n",
    "#         self.timestamp = timestamp\n",
    "\n",
    "# # Define Pydantic models\n",
    "# class ClaimModel(BaseModel):\n",
    "#     claim: str\n",
    "#     measurable: bool\n",
    "#     analysis: str\n",
    "\n",
    "# class ClaimList(BaseModel):\n",
    "#     claims: List[ClaimModel]\n",
    "\n",
    "# # Create the output parser\n",
    "# parser = PydanticOutputParser(pydantic_object=ClaimList)\n",
    "\n",
    "# # Load the document\n",
    "# file_path = \"data/President_Bidens_State_of_the_Union_Address_2024-03-08T034913Z.txt\"\n",
    "# loader = TextLoader(file_path)\n",
    "# document = loader.load()\n",
    "\n",
    "# # Create an LLM\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# # Create a prompt to extract speaker from filename\n",
    "# extract_speaker_template = ChatPromptTemplate.from_template(\n",
    "#     \"Extract the speaker's name from this filename: {filename}. Only return the name, nothing else.\"\n",
    "# )\n",
    "\n",
    "# # Extract speaker using LLM\n",
    "# speaker_chain = extract_speaker_template | llm\n",
    "# speaker_result = speaker_chain.invoke({\"filename\": os.path.basename(file_path)})\n",
    "# speaker = speaker_result.content.strip()\n",
    "\n",
    "# # Get current timestamp\n",
    "# timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "# # Create the prompt templates for claim extraction and analysis\n",
    "# extract_claims_template = ChatPromptTemplate.from_template(\n",
    "#     \"Extract claims from the following text:\\n\\n{text}\\n\\n list each claim as a separate bullet point. Claims:\"\n",
    "# )\n",
    "\n",
    "# analyze_claims_template = ChatPromptTemplate.from_template(\n",
    "#     \"\"\"For each claim, determine if it is measurable or not. If measurable, explain how it could be quantifiably measured or validated using real-world data. If not measurable, explain why it's too vague or subjective to measure. Consider specific metrics, data sources, or methods that could be used for validation.\n",
    "\n",
    "#     Format your response as a list of claims with their analysis, following this structure:\n",
    "#     {format_instructions}\n",
    "\n",
    "#     Claims to analyze:\n",
    "#     {claims}\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# # Create the LCEL chain for claim extraction and analysis\n",
    "# chain = (\n",
    "#     {\"text\": lambda x: x['text']}\n",
    "#     | extract_claims_template\n",
    "#     | llm\n",
    "#     | {\"claims\": lambda x: x.content}\n",
    "#     | analyze_claims_template.partial(format_instructions=parser.get_format_instructions())\n",
    "#     | llm\n",
    "#     | parser\n",
    "# )\n",
    "\n",
    "# # Run the chain\n",
    "# result = chain.invoke({\"text\": document[0].page_content})\n",
    "\n",
    "# # Convert the results to Claims objects\n",
    "# claims_objects = [\n",
    "#     Claims(\n",
    "#         speaker=speaker,\n",
    "#         claim=claim.claim,\n",
    "#         timestamp=timestamp,\n",
    "#         measurable=claim.measurable,\n",
    "#         analysis=claim.analysis\n",
    "#     )\n",
    "#     for claim in result.claims\n",
    "# ]\n",
    "# print(\"nunber of claims: \", len(claims_objects))\n",
    "# # Print the results\n",
    "# for claim in claims_objects:\n",
    "#     print(f\"Speaker: {claim.speaker}\")\n",
    "#     print(f\"Claim: {claim.claim}\")\n",
    "#     print(f\"Timestamp: {claim.timestamp}\")\n",
    "#     print(f\"Measurable: {'Yes' if claim.measurable else 'No'}\")\n",
    "#     print(f\"Analysis: {claim.analysis}\")\n",
    "#     print()\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.output_parsers import PydanticOutputParser\n",
    "# from pydantic import BaseModel, Field\n",
    "# from typing import List\n",
    "# import datetime\n",
    "# import os\n",
    "\n",
    "# # Import the Claims class\n",
    "# # from models.claims import Claims\n",
    "\n",
    "# class Claims:\n",
    "#     def __init__(self, speaker:str, claim:str, timestamp:str, measurable:bool, analysis:str, char_indices:tuple)->None:\n",
    "#         self.speaker = speaker\n",
    "#         self.claim = claim\n",
    "#         self.measurable = measurable\n",
    "#         self.analysis = analysis\n",
    "#         self.timestamp = timestamp\n",
    "#         self.char_indices = char_indices\n",
    "\n",
    "# # Define a Pydantic model that matches the Claims class\n",
    "# class ClaimModel(BaseModel):\n",
    "#     speaker: str\n",
    "#     claim: str\n",
    "#     timestamp: str\n",
    "#     measurable: bool\n",
    "#     analysis: str\n",
    "#     char_indices: str\n",
    "\n",
    "\n",
    "# class ClaimList(BaseModel):\n",
    "#     claims: List[ClaimModel]\n",
    "\n",
    "# # Create the output parser\n",
    "# parser = PydanticOutputParser(pydantic_object=ClaimList)\n",
    "\n",
    "# # Load the document\n",
    "# file_path = \"data/President_Bidens_State_of_the_Union_Address_2024-03-08T034913Z.txt\"\n",
    "# loader = TextLoader(file_path)\n",
    "# document = loader.load()\n",
    "\n",
    "# # Get current timestamp\n",
    "# timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "# # Create a prompt to extract speaker from filename\n",
    "# extract_speaker_template = ChatPromptTemplate.from_template(\n",
    "#     \"Extract the speaker's name from this filename: {filename}. Only return the name, nothing else.\"\n",
    "# )\n",
    "\n",
    "# # Extract speaker using LLM\n",
    "# speaker_chain = extract_speaker_template | llm\n",
    "# speaker_result = speaker_chain.invoke({\"filename\": os.path.basename(file_path)})\n",
    "# speaker = speaker_result.content.strip()\n",
    "\n",
    "# # Create the prompt templates\n",
    "# extract_claims_template = ChatPromptTemplate.from_template(\n",
    "#     \"Extract claims from the following text:\\n\\n{text}\\n\\n For each claim, list it as a separate bullet point and include the starting and ending character indices in parentheses. Claims:\"\n",
    "# )\n",
    "\n",
    "\n",
    "# analyze_claims_template = ChatPromptTemplate.from_template(\n",
    "#     \"\"\"For each claim, determine if it is measurable or not. If measurable, explain how it could be quantifiably measured or validated using real-world data. If not measurable, explain why it's too vague or subjective to measure. Consider specific metrics, data sources, or methods that could be used for validation.\n",
    "\n",
    "#     Format your response as a list of claims with their analysis, following this structure:\n",
    "#     {format_instructions}\n",
    "\n",
    "#     Claims to analyze:\n",
    "#     {claims}\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # Create the LCEL chain\n",
    "# chain = (\n",
    "#     {\"text\": lambda x: x['text']}\n",
    "#     | extract_claims_template\n",
    "#     | llm\n",
    "#     | (lambda x: {\"claims\": [{\"claim\": c.split(\"(\")[0], \"char_indices\": c.split(\"(\")[1].split(\")\")[0]} for c in x.content.split(\"\\n\") if \"(\" in c and \")\" in c]})\n",
    "#     | analyze_claims_template.partial(format_instructions=parser.get_format_instructions())\n",
    "#     | llm\n",
    "#     | parser\n",
    "# )\n",
    "\n",
    "\n",
    "# # Run the chain\n",
    "# result = chain.invoke({\"text\": document[0].page_content})\n",
    "\n",
    "# # Convert the results to Claims objects\n",
    "# claims_objects = [\n",
    "#     Claims(\n",
    "#         speaker=speaker,\n",
    "#         claim=claim.claim,\n",
    "#         timestamp=timestamp,\n",
    "#         measurable=claim.measurable,\n",
    "#         analysis=claim.analysis,\n",
    "#         char_indices=claim.char_indices\n",
    "#     )\n",
    "#     for claim in result.claims\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Print the results\n",
    "# for claim in claims_objects:\n",
    "#     print(f\"Speaker: {claim.speaker}\")\n",
    "#     print(f\"Claim: {claim.claim}\")\n",
    "#     print(f\"Timestamp: {claim.timestamp}\")\n",
    "#     print(f\"Measurable: {'Yes' if claim.measurable else 'No'}\")\n",
    "#     print(f\"Analysis: {claim.analysis}\")\n",
    "#     print(f\"char_indices:{claim.char_indices}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
